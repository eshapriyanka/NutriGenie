{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f079163",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-02-17T13:33:47.715192Z",
          "iopub.status.busy": "2023-02-17T13:33:47.714740Z",
          "iopub.status.idle": "2023-02-17T13:33:57.255131Z",
          "shell.execute_reply": "2023-02-17T13:33:57.254011Z"
        },
        "papermill": {
          "duration": 9.551122,
          "end_time": "2023-02-17T13:33:57.257793",
          "exception": false,
          "start_time": "2023-02-17T13:33:47.706671",
          "status": "completed"
        },
        "tags": [],
        "id": "8f079163"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be05272",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:33:57.272785Z",
          "iopub.status.busy": "2023-02-17T13:33:57.271449Z",
          "iopub.status.idle": "2023-02-17T13:33:57.277196Z",
          "shell.execute_reply": "2023-02-17T13:33:57.276267Z"
        },
        "papermill": {
          "duration": 0.015274,
          "end_time": "2023-02-17T13:33:57.279378",
          "exception": false,
          "start_time": "2023-02-17T13:33:57.264104",
          "status": "completed"
        },
        "tags": [],
        "id": "6be05272"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff7582eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:33:57.292669Z",
          "iopub.status.busy": "2023-02-17T13:33:57.291985Z",
          "iopub.status.idle": "2023-02-17T13:33:57.296657Z",
          "shell.execute_reply": "2023-02-17T13:33:57.295744Z"
        },
        "papermill": {
          "duration": 0.013451,
          "end_time": "2023-02-17T13:33:57.298666",
          "exception": false,
          "start_time": "2023-02-17T13:33:57.285215",
          "status": "completed"
        },
        "tags": [],
        "id": "ff7582eb"
      },
      "outputs": [],
      "source": [
        "model_save_path = './diabatasty'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b82f57",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:33:57.311888Z",
          "iopub.status.busy": "2023-02-17T13:33:57.311026Z",
          "iopub.status.idle": "2023-02-17T13:34:24.634146Z",
          "shell.execute_reply": "2023-02-17T13:34:24.633132Z"
        },
        "papermill": {
          "duration": 27.331954,
          "end_time": "2023-02-17T13:34:24.636496",
          "exception": false,
          "start_time": "2023-02-17T13:33:57.304542",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b82f57",
        "outputId": "8586c6cc-5e73-4c05-9a41-56b519138d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50260, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
        "                                              bos_token='<|startoftext|>',\n",
        "                                              eos_token='<|endoftext|>',\n",
        "                                              unk_token='<|unknown|>',\n",
        "                                              pad_token='<|pad|>'\n",
        "                                             )\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1da0a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.651601Z",
          "iopub.status.busy": "2023-02-17T13:34:24.651267Z",
          "iopub.status.idle": "2023-02-17T13:34:24.770213Z",
          "shell.execute_reply": "2023-02-17T13:34:24.769172Z"
        },
        "papermill": {
          "duration": 0.128834,
          "end_time": "2023-02-17T13:34:24.772571",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.643737",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a1da0a5",
        "outputId": "4350401a-ca86-47e9-91a0-b89ebaa9b191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./diabatasty/tokenizer_config.json',\n",
              " './diabatasty/special_tokens_map.json',\n",
              " './diabatasty/vocab.json',\n",
              " './diabatasty/merges.txt',\n",
              " './diabatasty/added_tokens.json',\n",
              " './diabatasty/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer.save_pretrained(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc39f5ae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.786887Z",
          "iopub.status.busy": "2023-02-17T13:34:24.786577Z",
          "iopub.status.idle": "2023-02-17T13:34:24.793202Z",
          "shell.execute_reply": "2023-02-17T13:34:24.792157Z"
        },
        "papermill": {
          "duration": 0.01732,
          "end_time": "2023-02-17T13:34:24.796348",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.779028",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc39f5ae",
        "outputId": "8a259483-cff7-4b9b-acef-d1cede3b15cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50259]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23756709",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.811848Z",
          "iopub.status.busy": "2023-02-17T13:34:24.811588Z",
          "iopub.status.idle": "2023-02-17T13:34:24.816657Z",
          "shell.execute_reply": "2023-02-17T13:34:24.815723Z"
        },
        "papermill": {
          "duration": 0.014564,
          "end_time": "2023-02-17T13:34:24.818777",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.804213",
          "status": "completed"
        },
        "tags": [],
        "id": "23756709"
      },
      "outputs": [],
      "source": [
        "def generate(prompt):\n",
        "    inputs = tokenizer.encode_plus(prompt, return_tensors='pt')\n",
        "    output = model.generate(**inputs,max_length=256,do_sample=True,pad_token_id=50259)\n",
        "    print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a6c391",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.833203Z",
          "iopub.status.busy": "2023-02-17T13:34:24.832459Z",
          "iopub.status.idle": "2023-02-17T13:34:24.839003Z",
          "shell.execute_reply": "2023-02-17T13:34:24.838056Z"
        },
        "papermill": {
          "duration": 0.015739,
          "end_time": "2023-02-17T13:34:24.841069",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.825330",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a6c391",
        "outputId": "5b7cfa89-52c1-44e6-885d-0d51e395aead"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '<|startoftext|>',\n",
              " 'eos_token': '<|endoftext|>',\n",
              " 'unk_token': '<|unknown|>',\n",
              " 'pad_token': '<|pad|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e602b365",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.855603Z",
          "iopub.status.busy": "2023-02-17T13:34:24.854828Z",
          "iopub.status.idle": "2023-02-17T13:34:24.860740Z",
          "shell.execute_reply": "2023-02-17T13:34:24.859815Z"
        },
        "papermill": {
          "duration": 0.015314,
          "end_time": "2023-02-17T13:34:24.862892",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.847578",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e602b365",
        "outputId": "5ffe6f28-44f4-4560-8bef-0f1d9ad4dceb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50257]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer.convert_tokens_to_ids(['<|startoftext|>'],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7206fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:24.877711Z",
          "iopub.status.busy": "2023-02-17T13:34:24.876792Z",
          "iopub.status.idle": "2023-02-17T13:34:25.101607Z",
          "shell.execute_reply": "2023-02-17T13:34:25.100498Z"
        },
        "papermill": {
          "duration": 0.234701,
          "end_time": "2023-02-17T13:34:25.104154",
          "exception": false,
          "start_time": "2023-02-17T13:34:24.869453",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "ba7206fb",
        "outputId": "6f24ef45-b5a4-4c60-b688-5103d0475307"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                TranslatedRecipeName  \\\n",
              "0  Keema Bhindi Masala Recipe - Okra & Minced Mea...   \n",
              "1                    Vegetarian Grilled Gyros Recipe   \n",
              "2                      Whole Wheat Palak Naan Recipe   \n",
              "3                      Savoury Yogurt Parfait Recipe   \n",
              "4  Cheesy Veg Paniyaram Recipe With Cheesy Tomato...   \n",
              "\n",
              "                               TranslatedIngredients  TotalTimeInMins  \\\n",
              "0  250 grams Bhindi (Lady Finger/Okra) - cut into...               45   \n",
              "1  1 cup Button mushrooms - (use portabella mushr...               55   \n",
              "2  1 cup Curd (Dahi / Yogurt),1 teaspoon Baking s...               50   \n",
              "3  Mint Leaves (Pudina) - chopped (you can also u...               20   \n",
              "4  2 cups Bajra Dosa Batter,1 Carrot (Gajjar) - g...               25   \n",
              "\n",
              "                Cuisine                                       instructions  \\\n",
              "0  North Indian Recipes  To begin making the Keema Bhindi Masala Recipe...   \n",
              "1                 Greek  To begin making the Vegetarian Grilled Gyros r...   \n",
              "2  North Indian Recipes  To begin making the Whole Wheat Palak Naan, fi...   \n",
              "3           Continental  To begin making the Savoury Yogurt Parfait Rec...   \n",
              "4  South Indian Recipes  To begin making Cheesy Veg Paniyaram Recipe fi...   \n",
              "\n",
              "                                                 URL  \\\n",
              "0  https://www.archanaskitchen.com/keema-bhindi-r...   \n",
              "1  https://www.archanaskitchen.com/vegetarian-gri...   \n",
              "2  https://www.archanaskitchen.com/whole-wheat-pa...   \n",
              "3  https://www.archanaskitchen.com/savoury-yogurt...   \n",
              "4  https://www.archanaskitchen.com/cheesy-veg-pan...   \n",
              "\n",
              "                                         ingredients  \\\n",
              "0  tomato,salt,coriander (dhania),ginger,mustard ...   \n",
              "1  yellow bell pepper (capsicum),tomato,red bell ...   \n",
              "2  salt,wheat flour,kalonji (onion nigella seeds)...   \n",
              "3  mint leaves (pudina) (you can also basilcorian...   \n",
              "4  tomato,salt,sunflower oil,green chilli,bajra d...   \n",
              "\n",
              "                                           image-url  Ingredient-count  \n",
              "0  https://www.archanaskitchen.com/images/archana...                15  \n",
              "1  https://www.archanaskitchen.com/images/archana...                17  \n",
              "2  https://www.archanaskitchen.com/images/archana...                 9  \n",
              "3  https://www.archanaskitchen.com/images/archana...                13  \n",
              "4  https://www.archanaskitchen.com/images/archana...                16  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f01a26d-04a9-4b5d-abcc-c38cc5b7bf6c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TranslatedRecipeName</th>\n",
              "      <th>TranslatedIngredients</th>\n",
              "      <th>TotalTimeInMins</th>\n",
              "      <th>Cuisine</th>\n",
              "      <th>instructions</th>\n",
              "      <th>URL</th>\n",
              "      <th>ingredients</th>\n",
              "      <th>image-url</th>\n",
              "      <th>Ingredient-count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Keema Bhindi Masala Recipe - Okra &amp; Minced Mea...</td>\n",
              "      <td>250 grams Bhindi (Lady Finger/Okra) - cut into...</td>\n",
              "      <td>45</td>\n",
              "      <td>North Indian Recipes</td>\n",
              "      <td>To begin making the Keema Bhindi Masala Recipe...</td>\n",
              "      <td>https://www.archanaskitchen.com/keema-bhindi-r...</td>\n",
              "      <td>tomato,salt,coriander (dhania),ginger,mustard ...</td>\n",
              "      <td>https://www.archanaskitchen.com/images/archana...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vegetarian Grilled Gyros Recipe</td>\n",
              "      <td>1 cup Button mushrooms - (use portabella mushr...</td>\n",
              "      <td>55</td>\n",
              "      <td>Greek</td>\n",
              "      <td>To begin making the Vegetarian Grilled Gyros r...</td>\n",
              "      <td>https://www.archanaskitchen.com/vegetarian-gri...</td>\n",
              "      <td>yellow bell pepper (capsicum),tomato,red bell ...</td>\n",
              "      <td>https://www.archanaskitchen.com/images/archana...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Whole Wheat Palak Naan Recipe</td>\n",
              "      <td>1 cup Curd (Dahi / Yogurt),1 teaspoon Baking s...</td>\n",
              "      <td>50</td>\n",
              "      <td>North Indian Recipes</td>\n",
              "      <td>To begin making the Whole Wheat Palak Naan, fi...</td>\n",
              "      <td>https://www.archanaskitchen.com/whole-wheat-pa...</td>\n",
              "      <td>salt,wheat flour,kalonji (onion nigella seeds)...</td>\n",
              "      <td>https://www.archanaskitchen.com/images/archana...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Savoury Yogurt Parfait Recipe</td>\n",
              "      <td>Mint Leaves (Pudina) - chopped (you can also u...</td>\n",
              "      <td>20</td>\n",
              "      <td>Continental</td>\n",
              "      <td>To begin making the Savoury Yogurt Parfait Rec...</td>\n",
              "      <td>https://www.archanaskitchen.com/savoury-yogurt...</td>\n",
              "      <td>mint leaves (pudina) (you can also basilcorian...</td>\n",
              "      <td>https://www.archanaskitchen.com/images/archana...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cheesy Veg Paniyaram Recipe With Cheesy Tomato...</td>\n",
              "      <td>2 cups Bajra Dosa Batter,1 Carrot (Gajjar) - g...</td>\n",
              "      <td>25</td>\n",
              "      <td>South Indian Recipes</td>\n",
              "      <td>To begin making Cheesy Veg Paniyaram Recipe fi...</td>\n",
              "      <td>https://www.archanaskitchen.com/cheesy-veg-pan...</td>\n",
              "      <td>tomato,salt,sunflower oil,green chilli,bajra d...</td>\n",
              "      <td>https://www.archanaskitchen.com/images/archana...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f01a26d-04a9-4b5d-abcc-c38cc5b7bf6c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f01a26d-04a9-4b5d-abcc-c38cc5b7bf6c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f01a26d-04a9-4b5d-abcc-c38cc5b7bf6c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5de5b165-d003-4d76-9afe-809f36d2d46e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5de5b165-d003-4d76-9afe-809f36d2d46e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5de5b165-d003-4d76-9afe-809f36d2d46e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clean",
              "summary": "{\n  \"name\": \"clean\",\n  \"rows\": 5938,\n  \"fields\": [\n    {\n      \"column\": \"TranslatedRecipeName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5928,\n        \"samples\": [\n          \"Anda Bhurji Recipe\",\n          \"Cekodok / Pick Up Banana Recipe (Vegan Fried Banana Balls)\",\n          \" Chettinad Dry Pepper Chicken Recipe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TranslatedIngredients\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5934,\n        \"samples\": [\n          \"100 grams Button mushrooms - quartered,1 tablespoon Extra Virgin Olive Oil,1/4 cup Sweet corn - steamed,1/2 cup Heavy whipping cream,1/4 cup Carrots (Gajjar) - finely chopped,Salt and Pepper - as needed,1/8 teaspoon Nutmeg,1 Parsley leaves,Handful Walnuts - toasted,1/2 cup Vegetable stock,1 tablespoon Butter,2 tablespoons Parmesan cheese,3 Egg yolks,1/4 cup Green peas (Matar) - steamed,1/2 teaspoon Paprika powder,2 cloves Garlic - finely choped,1 teaspoon Black pepper powder\",\n          \"1 pinch Salt,10 Cardamom (Elaichi) Pods/Seeds - peeled and powdered,1-1/2 cups Whole Wheat Flour,100 grams Butter (unsalted) - chilled and cubed,1/4 cup Milk,8 Cashew nuts - halved lengthwise,1/4 cup Wheat Bran,1/2 cup Caster Sugar\",\n          \"1/2 teaspoon Mustard seeds,3 teaspoons Sambar Powder,Water - as required,Salt - to taste,2 teaspoons Jaggery - powdered,1/2 cup Kala Chana (Brown Chickpeas),30 grams Tamarind,1/4 teaspoon Methi Seeds (Fenugreek Seeds),14 cup Fresh coconut - grated,1 sprig Curry leaves,1 tablespoon Sesame (Gingelly) Oil\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TotalTimeInMins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 86,\n        \"min\": 0,\n        \"max\": 2925,\n        \"num_unique_values\": 161,\n        \"samples\": [\n          830,\n          29,\n          415\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cuisine\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 82,\n        \"samples\": [\n          \"Chettinad\",\n          \"North Indian Recipes\",\n          \"Chinese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instructions\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5937,\n        \"samples\": [\n          \"To begin making the Karela Thepla Recipe, in a large mixing bowl, add wheat flour and besan along with grated karela, ajwain, turmeric, red chili powder, cumin powder, green chili paste, ginger paste, ghee/oil and mix well with a spoon or fork.Now slowly add warm water and knead the flour mix to make a soft dough.Heat a flat bottomed tawa and meanwhile prepare the theplas.\\nPinch a lemon sized ball from the dough and roll out to make a thin roti on the dusted floor.Once the tawa is hot, place the thepla on the tawa and dry roast it from both sides.Now brush some melted butter on both sides of the thepla one by one and roast till golden in colour.Transfer the thepla on a serving plate and serve hot.\\nServe Karela Thepla along with Tomato Onion Cucumber Raita and Aam Ka Achaar for a light lunch or breakfast.\\n\",\n          \"To begin making the Aloo Gobi Kathi Roll, we have to first cook the aloo and gobi.\\nInto a pressure cooker, add the potatoes along with 1/2 cup of water and pressure cook for 3 to 4 whistles and turn off the heat.\\nAllow the pressure to release naturally.Peel the skin of the potatoes and keep aside.If you have a steamer, steam the cauliflower florets on high heat for 3 to 4 minutes until just about cooked, but its still firm.\\nYou can also pan roast the cauliflower with a little salt until it has lightly cooked.In a bowl add curd 1/2 cup yogurt along with chaat masala, red chilli powder, turmeric powder and salt.\\nAdd the baby potatoes and cauliflower florets into the curd mixture and coat the aloo gobi evenly using a spoon.\\nCheck for seasoning and leave aside for 15 minutes to marinate.\\n Now insert the baby potatoes and cauliflowers into a sticks or skewers for roasting on a skillet.\\nPlace a skillet on medium heat and add a teaspoon of oil.\\nAdd cumin seeds and let them crackle.\\nPlace the skewered aloo gobi on the skillet and roast it until you see a few brown spots on the vegetables and see it getting roasted.Roast all the skewers on a low to medium heat and keep aside.\\n*Making the paratha for the Kati RollsIn a bowl mix whole wheat flour, salt and one teaspoon of oil and mix well.\\nAdd little water at a time and bring the flour together to form a smooth dough.\\nCover with a muslin cloth and keep aside for 10 minutes.\\nMake equal portions of the dough and roll it into balls.\\nOn a clean surface dust some flour and flatten the ball of dough on it.\\nRoll out the dough into a paratha.\\nPlace a roti tawa on medium heat and cook all the parathas with ghee or oil.\\nKeep the cooked parathas aside.\\nIn a medium bowl, mix together sliced onions and capsicum slices along with lemon juice, salt and green chillies.*Assembling the Kathi RollsPlace a paratha on foil paper.\\nPlace a small portion of the onion and capsicum slices.\\nNext place a skewer of roasted tandoori aloo gobi in the centre.\\nPull the skewer out and discard.\\nRoll the parathas along with the foil and prepare the rest of the Tandoori Aloo Gobi Kathi rolls the similar way.\\nServe Tandoori Aloo Gobi Kathi Rolls along with a Dhaniya Pudina Chutney and a cup of Masala Chai for a tea time meal or pack these Aloo Gobi Kathi Rolls for lunch along with a cup of curd.\\n\",\n          \"To begin making the Sprouted Moong Dhokla recipe, wash the whole moong and soak them in water for overnight.Rinse again the soaked Moong and place them in a wet muslin cloth and again put them for 6 to 8 hours without disturbing the position.Untie the cloth and check the Moong, The sprouted Moong are ready to use.\\nIf they are not sprouted properly, sprinkle some water on them and again tie the cloth for 3 to 4 more hours.Now to make dhokla, first crush the sprouted Moong coarsely and put the paste aside.\\n(Note: To crush the Moong, you can use little amount of water to make coarse paste.)Now, cut the carrot roughly, and add green chilies, ginger and some roughly chopped coriander leaves and make churn them in a mixer grinder.Add this carrot mixture paste in sprouted Moong paste and make a Dhokla batter by adding buttermilk and Gram flour.Season the batter with salt, red chili powder, and cumin seed powder and keep the batter aside.Now heat a dhokla maker with some amount of water in it, and grease the dhokla plate with oil.Let the water starts boiling, Now add Soda bicarb in dhokla batter and 1 tablespoon of oil and mix the batter properly, and place this mixture on dhokla plate, cover it with lid and steam the dhokla at least for 15-20 minutes.If you don't have dhokla maker, simply take a big deep vessel and add water in it, let it starts boiling and place the plate on it which has the exact diameter of that vessel, and spread the dhokla mixture in that plate and cover it with another big size of plate tied with muslin cloth.Serve Sprouted Moong Dhokla along with Dhaniya Pudina Chutney for breakfast or serve as a tea time snack.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5938,\n        \"samples\": [\n          \"https://www.archanaskitchen.com/coorgi-style-chicken-curry-recipe\",\n          \"https://www.archanaskitchen.com/khatti-meethi-tinda-sabzi-recipe-in-hindi\",\n          \"https://www.archanaskitchen.com/gan-bian-dou-jiao-recipe-dry-fried-sichuan-string-beans\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ingredients\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5932,\n        \"samples\": [\n          \"salt,vegetable stock,cloves garlic choped,nutmeg,heavy whipping cream,parmesan cheese,paprika powder,black pepper powder,eggs,green peas (matar),virgin olive oil,carrot,parsley leaves,sweet corn,pepper,button mushrooms,butter,walnuts\",\n          \"cashew nuts,salt,wheat flour,wheat bran,cardamom (elaichi) podsseeds,milk,caster sugar,butter\",\n          \"ajwain (carom seeds),cloves garlic,onion,soy (nuggets),turmeric powder,green chillies,salt,cardamom (elaichi) podsseeds,mint leaves (pudina),star anise,garam masala powder,red chilli powder,curd,button mushrooms,ginger,ghee,bay leaf (tej patta),biryani masala,basmati rice spices infused water (spices include cinnamon,cinnamon stick (dalchini),cloves (laung)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image-url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5653,\n        \"samples\": [\n          \"https://www.archanaskitchen.com/images/archanaskitchen/1-Author/Madhuli_Ajay/Spicy_Roti_Bowls.jpg\",\n          \"https://www.archanaskitchen.com/images/archanaskitchen/10-Brands/Del_Monte_Mayo/Carrot-Corn-Sandwich-Spread-Mayo-Pancakes2.jpg\",\n          \"https://www.archanaskitchen.com/images/archanaskitchen/0-Archanas-Kitchen-Recipes/2018/April-18/Jowar_Atta_Pyaz_Thepla_Jowar_Onion_Paratha-3.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ingredient-count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 3,\n        \"max\": 34,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          34,\n          22,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "clean = pd.read_csv('/content/Cleaned_Indian_Food_Dataset.csv')\n",
        "clean = clean.sample(frac=1)\n",
        "clean.reset_index(drop=True,inplace=True)\n",
        "clean.rename(columns={'Cleaned-Ingredients': 'ingredients', 'TranslatedInstructions': 'instructions'}, inplace=True)\n",
        "\n",
        "clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7fb80b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.119728Z",
          "iopub.status.busy": "2023-02-17T13:34:25.119058Z",
          "iopub.status.idle": "2023-02-17T13:34:25.124333Z",
          "shell.execute_reply": "2023-02-17T13:34:25.123239Z"
        },
        "papermill": {
          "duration": 0.015245,
          "end_time": "2023-02-17T13:34:25.126409",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.111164",
          "status": "completed"
        },
        "tags": [],
        "id": "de7fb80b"
      },
      "outputs": [],
      "source": [
        "def print_recipe(idx):\n",
        "    print(f\"{clean['ingredients'][idx]}\\n\\n{clean['instructions'][idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa48926",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.142078Z",
          "iopub.status.busy": "2023-02-17T13:34:25.141623Z",
          "iopub.status.idle": "2023-02-17T13:34:25.146115Z",
          "shell.execute_reply": "2023-02-17T13:34:25.145183Z"
        },
        "papermill": {
          "duration": 0.014662,
          "end_time": "2023-02-17T13:34:25.148143",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.133481",
          "status": "completed"
        },
        "tags": [],
        "id": "0aa48926"
      },
      "outputs": [],
      "source": [
        "def form_string(ingredient,instruction):\n",
        "    s = f\"<|startoftext|>Ingredients:\\n{ingredient.strip()}\\n\\nInstructions:\\n{instruction.strip()}<|endoftext|>\"\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean = clean.sample(n=100, random_state=1)"
      ],
      "metadata": {
        "id": "49I4hscCC20f"
      },
      "id": "49I4hscCC20f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320c6c26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.163423Z",
          "iopub.status.busy": "2023-02-17T13:34:25.162538Z",
          "iopub.status.idle": "2023-02-17T13:34:25.250755Z",
          "shell.execute_reply": "2023-02-17T13:34:25.249871Z"
        },
        "papermill": {
          "duration": 0.098057,
          "end_time": "2023-02-17T13:34:25.253001",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.154944",
          "status": "completed"
        },
        "tags": [],
        "id": "320c6c26"
      },
      "outputs": [],
      "source": [
        "data = clean.apply(lambda x:form_string(x['ingredients'],x['instructions']),axis=1).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f07ca2e",
      "metadata": {
        "papermill": {
          "duration": 0.006557,
          "end_time": "2023-02-17T13:34:25.266255",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.259698",
          "status": "completed"
        },
        "tags": [],
        "id": "5f07ca2e"
      },
      "source": [
        "https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f1d0f1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.281715Z",
          "iopub.status.busy": "2023-02-17T13:34:25.281434Z",
          "iopub.status.idle": "2023-02-17T13:34:25.286154Z",
          "shell.execute_reply": "2023-02-17T13:34:25.285200Z"
        },
        "papermill": {
          "duration": 0.01452,
          "end_time": "2023-02-17T13:34:25.288208",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.273688",
          "status": "completed"
        },
        "tags": [],
        "id": "55f1d0f1"
      },
      "outputs": [],
      "source": [
        "train_size = 0.85\n",
        "train_len = int(train_size * len(data))\n",
        "train_data = data[:train_len]\n",
        "val_data = data[train_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42c0ce7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.303272Z",
          "iopub.status.busy": "2023-02-17T13:34:25.302446Z",
          "iopub.status.idle": "2023-02-17T13:34:25.309416Z",
          "shell.execute_reply": "2023-02-17T13:34:25.308588Z"
        },
        "papermill": {
          "duration": 0.016565,
          "end_time": "2023-02-17T13:34:25.311351",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.294786",
          "status": "completed"
        },
        "tags": [],
        "id": "b42c0ce7"
      },
      "outputs": [],
      "source": [
        "class RecipeDataset:\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for item in tqdm(data):\n",
        "            encodings = tokenizer.encode_plus(item,\n",
        "                                              truncation=True,\n",
        "                                              padding='max_length',\n",
        "                                              max_length=1024,\n",
        "                                              return_tensors='pt'\n",
        "                                             )\n",
        "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
        "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a70118",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.325646Z",
          "iopub.status.busy": "2023-02-17T13:34:25.325128Z",
          "iopub.status.idle": "2023-02-17T13:34:25.330285Z",
          "shell.execute_reply": "2023-02-17T13:34:25.329384Z"
        },
        "papermill": {
          "duration": 0.014469,
          "end_time": "2023-02-17T13:34:25.332318",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.317849",
          "status": "completed"
        },
        "tags": [],
        "id": "d3a70118"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([item[0] for item in batch]),\n",
        "        'attention_mask': torch.stack([item[1] for item in batch]),\n",
        "        'labels': torch.stack([item[0] for item in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aafedd7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:25.347030Z",
          "iopub.status.busy": "2023-02-17T13:34:25.346277Z",
          "iopub.status.idle": "2023-02-17T13:34:32.446178Z",
          "shell.execute_reply": "2023-02-17T13:34:32.445186Z"
        },
        "papermill": {
          "duration": 7.109639,
          "end_time": "2023-02-17T13:34:32.448516",
          "exception": false,
          "start_time": "2023-02-17T13:34:25.338877",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "bbe0eadca68f40bb991a8aeecf05ebc7",
            "b4ec896c5f1d40969353fa839c2374e4",
            "b7060a52070144188dd1204f5ba13e03",
            "65628dc09e76437189e48ffecc550dad",
            "7f5e1ca5ef89483dab4d152f3b8110ba",
            "43ed75de80f0410a8b76ed81b33d3795",
            "2a2d975c2cf84f02a1b150b0102ee26f",
            "6debed80914d4724a87f3678b9dfa15a",
            "0e42af46defe463ab32bac514b3f3d94",
            "9604aae57dbc480688907ed31fa79a20",
            "dc446c9b5c41477aa57bc9e2cd5719c0",
            "e820966674a44777893b583d2323fb62",
            "ad5ec44774914c7ca0479e63dc5c6b04",
            "ae81d2f13d2a413d8f8c4bc3a0f46a95",
            "2a2899fe997b48f8916c93dc94726faa",
            "60af207cf60e43efaf16258884c955f7",
            "cd00de1d250b440d82cb6fc54e2e36e3",
            "b984e50058a34ccd90a12e55b2493ac9",
            "8c7d857bb3cc470db21e04e6e6772e62",
            "7cd4e629c72b4e4eb44cd2fc7b7e5c29",
            "03f12f7bd7b84c248e5bd83f6bb1b90e",
            "1da25bab470b42ef92b528d987a2b2d5"
          ]
        },
        "id": "7aafedd7",
        "outputId": "ca10fac3-2da5-4de7-b0cd-03b1e59f89f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/85 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbe0eadca68f40bb991a8aeecf05ebc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e820966674a44777893b583d2323fb62"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_ds = RecipeDataset(train_data)\n",
        "val_ds = RecipeDataset(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9217aa6a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:32.464813Z",
          "iopub.status.busy": "2023-02-17T13:34:32.464479Z",
          "iopub.status.idle": "2023-02-17T13:34:32.592945Z",
          "shell.execute_reply": "2023-02-17T13:34:32.591982Z"
        },
        "papermill": {
          "duration": 0.139256,
          "end_time": "2023-02-17T13:34:32.595349",
          "exception": false,
          "start_time": "2023-02-17T13:34:32.456093",
          "status": "completed"
        },
        "tags": [],
        "id": "9217aa6a"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(output_dir=model_save_path,\n",
        "                         per_device_train_batch_size=2,\n",
        "                         per_device_eval_batch_size=2,\n",
        "                         gradient_accumulation_steps=2,\n",
        "                         report_to='none',\n",
        "                         num_train_epochs=1,\n",
        "                         save_strategy='no'\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a63414d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:32.612549Z",
          "iopub.status.busy": "2023-02-17T13:34:32.610763Z",
          "iopub.status.idle": "2023-02-17T13:34:32.617663Z",
          "shell.execute_reply": "2023-02-17T13:34:32.616677Z"
        },
        "papermill": {
          "duration": 0.017367,
          "end_time": "2023-02-17T13:34:32.619942",
          "exception": false,
          "start_time": "2023-02-17T13:34:32.602575",
          "status": "completed"
        },
        "tags": [],
        "id": "8a63414d"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.AdamW(model.parameters(),lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,20,eta_min=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb46f00",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:32.635197Z",
          "iopub.status.busy": "2023-02-17T13:34:32.634926Z",
          "iopub.status.idle": "2023-02-17T13:34:37.810671Z",
          "shell.execute_reply": "2023-02-17T13:34:37.809610Z"
        },
        "papermill": {
          "duration": 5.186167,
          "end_time": "2023-02-17T13:34:37.813033",
          "exception": false,
          "start_time": "2023-02-17T13:34:32.626866",
          "status": "completed"
        },
        "tags": [],
        "id": "efb46f00"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model,\n",
        "                  args,\n",
        "                  train_dataset=train_ds,\n",
        "                  eval_dataset=val_ds,\n",
        "                  data_collator=collate_fn,\n",
        "                  optimizers=(optim,scheduler)\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede95bb7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T13:34:37.830145Z",
          "iopub.status.busy": "2023-02-17T13:34:37.828656Z",
          "iopub.status.idle": "2023-02-17T14:27:06.805075Z",
          "shell.execute_reply": "2023-02-17T14:27:06.804119Z"
        },
        "papermill": {
          "duration": 3148.986737,
          "end_time": "2023-02-17T14:27:06.807297",
          "exception": false,
          "start_time": "2023-02-17T13:34:37.820560",
          "status": "completed"
        },
        "tags": [],
        "id": "ede95bb7"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab7c07d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:06.825510Z",
          "iopub.status.busy": "2023-02-17T14:27:06.823784Z",
          "iopub.status.idle": "2023-02-17T14:27:07.793310Z",
          "shell.execute_reply": "2023-02-17T14:27:07.792280Z"
        },
        "papermill": {
          "duration": 0.9808,
          "end_time": "2023-02-17T14:27:07.795950",
          "exception": false,
          "start_time": "2023-02-17T14:27:06.815150",
          "status": "completed"
        },
        "tags": [],
        "id": "5ab7c07d",
        "outputId": "f960bb5a-41e3-4daf-9ddc-3033a7f68cf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./khaanaGPT\n",
            "Configuration saved in ./khaanaGPT/config.json\n",
            "Model weights saved in ./khaanaGPT/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dissco module"
      ],
      "metadata": {
        "id": "oOMdkjE6BwUW"
      },
      "id": "oOMdkjE6BwUW"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dissco(age, gender, hba1c, conditions):\n",
        "    # Gender factor\n",
        "    gender_factor = 1.4 if gender.lower() == 'female' else 1.0\n",
        "\n",
        "    # Base HDF score\n",
        "    hdf = 1.0\n",
        "\n",
        "    # Define condition impacts\n",
        "    condition_impacts = {\n",
        "        \"hypertension\": 0.5,\n",
        "        \"cardiomyopathy\": 1,\n",
        "        \"pericardial_disease\": 1,\n",
        "        \"myocarditis\": 1,\n",
        "        \"coronary_heart_disease\": 1,\n",
        "        \"peripheral_vascular_disease\": 2,\n",
        "        \"stent_or_bypass\": 3\n",
        "    }\n",
        "\n",
        "    # Add factors for each present condition\n",
        "    for condition in conditions:\n",
        "        hdf += condition_impacts.get(condition.lower(), 0)  # Ignore unrecognized conditions\n",
        "\n",
        "    # Calculate raw DissCo score\n",
        "    dissco_score = ((age / 10) + hba1c) * gender_factor * hdf\n",
        "\n",
        "    # Normalize the score (max possible score is 34.3)\n",
        "    max_score = 34.3\n",
        "    normalized_dissco = dissco_score / max_score\n",
        "\n",
        "    return normalized_dissco\n",
        "\n",
        "# Example usage:\n",
        "age = 60\n",
        "gender = \"male\"\n",
        "hba1c = 6\n",
        "conditions = [\"hypertension\", \"stent or bypass\"]\n",
        "\n",
        "normalized_dissco_score = calculate_dissco(age, gender, hba1c, conditions)\n",
        "print(\"Normalized DissCo Score:\", normalized_dissco_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73dyGaRcBv-M",
        "outputId": "e31dd3d0-4f43-4ee8-cc8c-385abb360101"
      },
      "id": "73dyGaRcBv-M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized DissCo Score: 0.5247813411078718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18GYyoynKu3g",
        "outputId": "12efe243-e22a-4b2d-c33b-7e7835c5d749"
      },
      "id": "18GYyoynKu3g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09197463",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:07.818182Z",
          "iopub.status.busy": "2023-02-17T14:27:07.817881Z",
          "iopub.status.idle": "2023-02-17T14:27:10.388460Z",
          "shell.execute_reply": "2023-02-17T14:27:10.387517Z"
        },
        "papermill": {
          "duration": 2.586384,
          "end_time": "2023-02-17T14:27:10.391206",
          "exception": false,
          "start_time": "2023-02-17T14:27:07.804822",
          "status": "completed"
        },
        "tags": [],
        "id": "09197463"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config"
      ],
      "metadata": {
        "id": "2zeo-jb9kTS9"
      },
      "id": "2zeo-jb9kTS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f756b8c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:10.408813Z",
          "iopub.status.busy": "2023-02-17T14:27:10.408506Z",
          "iopub.status.idle": "2023-02-17T14:27:12.965270Z",
          "shell.execute_reply": "2023-02-17T14:27:12.964200Z"
        },
        "papermill": {
          "duration": 2.568482,
          "end_time": "2023-02-17T14:27:12.968034",
          "exception": false,
          "start_time": "2023-02-17T14:27:10.399552",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f756b8c5",
        "outputId": "3c97fa83-b3e0-4399-aa3a-4ed634c01009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "pl = pipeline(task='text-generation',model='/content/drive/MyDrive/Colab Notebooks/assets/model', from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "try:\n",
        "    state_dict = torch.load('/content/sample_data/model/pytorch_model.bin')\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMM-JB9VKN7i",
        "outputId": "f875ab88-818b-4928-dd18-f0c152e51ab6"
      },
      "id": "QMM-JB9VKN7i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model: [Errno 2] No such file or directory: '/content/sample_data/model/pytorch_model.bin'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-e4190ea1d5ba>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/sample_data/model/pytorch_model.bin')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "import torch\n",
        "# Load the configuration from the config.json file\n",
        "config = GPT2Config.from_pretrained('/content/drive/MyDrive/Colab Notebooks/assets/model/config.json')  # Specify the path to your config file\n",
        "\n",
        "# Load the tokenizer (adjust based on your model, e.g., GPT2 or custom)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # Or a custom tokenizer if applicable\n",
        "\n",
        "# Load the model architecture and weights from the .bin file\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/assets/model',  # Replace with the path to the directory containing the .bin file\n",
        "    config=config\n",
        ")\n",
        "pl = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i06Db9sAD15",
        "outputId": "c9258098-0a42-436b-bb4e-a0b99160c015"
      },
      "id": "3i06Db9sAD15",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"potato, cheese, ginger\"\n",
        "result = pl(prompt,max_new_tokens=512,\n",
        "         penalty_alpha=0.6,\n",
        "         top_k=4,\n",
        "         pad_token_id=50259)\n",
        "\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuVUdpJqn9cm",
        "outputId": "be0e313f-d212-4c80-b207-a1a8ebae6688"
      },
      "id": "iuVUdpJqn9cm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "potato, cheese, ginger and oregano.\n",
            "\n",
            "Ingredients:\n",
            "salt\n",
            "cumin powder (jeera)\n",
            "red bell pepper (capsicum)\n",
            "ginger\n",
            "mustard seeds\n",
            "asafoetida (hing)\n",
            "curry leaves\n",
            "coriander (dhania) seeds\n",
            "cumin powder (jeera)\n",
            "green chillies\n",
            "turmeric powder\n",
            "\n",
            "Instructions:\n",
            "wash and wash the bell peppers and coriander leaves.heat oil in a heavy bottomed pan.\n",
            "add mustard seeds and allow it crackle.add curry leaves, red capsicum and allow it splutter.once the curry leaves have spluttered add asafoetida and saute for a minute and turn off the heat.\n",
            "allow the curry leaves to soften and add the chopped coriander leaves.now add the grated ginger, green chillies, turmeric powder, cumin powder, cumin powder, red chillies, coriander leaves.mix well and let the mixture simmer for about 30 seconds.after a few minutes, turn off the heat and allow the mixture  add the chopped coriander leaves, sprinkle some salt and mix well.\n",
            "let it simmer for a few more minutes.once the gravy is well combined add the chopped chilli and mix well.serve hot along with steamed rice and kachumber salad for a weekday meal.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "import torch\n",
        "# Load the configuration from the config.json file\n",
        "config = GPT2Config.from_pretrained('/content/sample_data/model/config.json')  # Specify the path to your config file\n",
        "\n",
        "# Load the tokenizer (adjust based on your model, e.g., GPT2 or custom)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # Or a custom tokenizer if applicable\n",
        "\n",
        "# Load the model architecture and weights from the .bin file\n",
        "# Try loading with 'latin1' encoding to handle potential encoding issues\n",
        "try:\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        '/content/sample_data/model',  # Replace with the path to the directory containing the .bin file\n",
        "        config=config,\n",
        "        # Specify 'latin1' encoding\n",
        "        _fast_init=False  # Force the model to load weights using torch.load\n",
        "    )\n",
        "except OSError as e:\n",
        "    if \"Unable to load weights\" in str(e):\n",
        "        print(\"Trying to load with 'latin1' encoding...\")\n",
        "        model = GPT2LMHeadModel.from_pretrained(\n",
        "            '/content/sample_data/model',\n",
        "            config=config,\n",
        "            _fast_init=False,  # Force the model to load weights using torch.load\n",
        "            torch_dtype=torch.float32 # Setting torch_dtype\n",
        "        )\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "pl = pipeline(task='text-generation', model=model, tokenizer=tokenizer)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TUSG_hZMCPJI",
        "outputId": "6a0e4d2a-3b1a-4713-b14b-0f348ad050a9"
      },
      "id": "TUSG_hZMCPJI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load with 'latin1' encoding...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to load weights from pytorch checkpoint file for '/content/sample_data/model/pytorch_model.bin' at '/content/sample_data/model/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mweights_only_kwarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_greater_or_equal_than_1_13\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"version\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                     raise OSError(\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-431cea8eb084>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     model = GPT2LMHeadModel.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;34m'/content/sample_data/model'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replace with the path to the directory containing the .bin file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3759\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3760\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/content/sample_data/model/pytorch_model.bin' at '/content/sample_data/model/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mweights_only_kwarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_greater_or_equal_than_1_13\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"version\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                     raise OSError(\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-431cea8eb084>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Unable to load weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trying to load with 'latin1' encoding...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         model = GPT2LMHeadModel.from_pretrained(\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;34m'/content/sample_data/model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3758\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3759\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3760\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     ) from e\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;34mf\"at '{checkpoint_file}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for '/content/sample_data/model/pytorch_model.bin' at '/content/sample_data/model/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9e19c7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:12.986821Z",
          "iopub.status.busy": "2023-02-17T14:27:12.986502Z",
          "iopub.status.idle": "2023-02-17T14:27:12.992078Z",
          "shell.execute_reply": "2023-02-17T14:27:12.990937Z"
        },
        "papermill": {
          "duration": 0.017297,
          "end_time": "2023-02-17T14:27:12.994402",
          "exception": false,
          "start_time": "2023-02-17T14:27:12.977105",
          "status": "completed"
        },
        "tags": [],
        "id": "ec9e19c7"
      },
      "outputs": [],
      "source": [
        "def create_prompt(ingredients):\n",
        "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
        "    ingredients = ingredients.strip().replace(',','\\n')\n",
        "    s = f\"<|startoftext|>Ingredients:\\n{ingredients}\\n\"\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc253319",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:13.012628Z",
          "iopub.status.busy": "2023-02-17T14:27:13.012201Z",
          "iopub.status.idle": "2023-02-17T14:27:13.016313Z",
          "shell.execute_reply": "2023-02-17T14:27:13.015358Z"
        },
        "papermill": {
          "duration": 0.015495,
          "end_time": "2023-02-17T14:27:13.018383",
          "exception": false,
          "start_time": "2023-02-17T14:27:13.002888",
          "status": "completed"
        },
        "tags": [],
        "id": "cc253319"
      },
      "outputs": [],
      "source": [
        "ingredients = ['Rice, Potatoes, Tomatoes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45df47cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-17T14:27:13.037080Z",
          "iopub.status.busy": "2023-02-17T14:27:13.036290Z",
          "iopub.status.idle": "2023-02-17T14:27:51.086157Z",
          "shell.execute_reply": "2023-02-17T14:27:51.085168Z"
        },
        "papermill": {
          "duration": 38.069886,
          "end_time": "2023-02-17T14:27:51.096864",
          "exception": false,
          "start_time": "2023-02-17T14:27:13.026978",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "45df47cf",
        "outputId": "aadf8959-f190-43f2-95ff-55c3c7989fb5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['from_pt'] (note: typos in the generate arguments will also show up in this list)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2cb58a512f3b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mingredients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     print(pl(prompt,\n\u001b[0m\u001b[1;32m      4\u001b[0m          \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mpenalty_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             )\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pull this out first, we only use it for stopping criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m         \u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_generation_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_assistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massistant_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munused_model_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1244\u001b[0m                 \u001b[0;34mf\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0;34m\" generate arguments will also show up in this list)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['from_pt'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ],
      "source": [
        "\n",
        "for ing in ingredients:\n",
        "    prompt = create_prompt(ing)\n",
        "    print(pl(prompt,\n",
        "         max_new_tokens=512,\n",
        "         penalty_alpha=0.6,\n",
        "         top_k=4,\n",
        "         pad_token_id=50259\n",
        "        )[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ing in ingredients:\n",
        "    prompt = create_prompt(ing)\n",
        "    print(pl(prompt,\n",
        "         max_new_tokens=512,\n",
        "         penalty_alpha=0.6,\n",
        "         top_k=4,\n",
        "         pad_token_id=50259\n",
        "        # Remove any unintended 'from_pt' argument from kwargs\n",
        "        )[0]['generated_text']) # Accessing the generated text from the output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Q6WxeeJZnUCT",
        "outputId": "f409cff4-680d-4f37-c578-86f44e5a33af"
      },
      "id": "Q6WxeeJZnUCT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The following `model_kwargs` are not used by the model: ['from_pt'] (note: typos in the generate arguments will also show up in this list)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d913baa144e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mingredients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     print(pl(prompt,\n\u001b[0m\u001b[1;32m      4\u001b[0m          \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mpenalty_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             )\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pull this out first, we only use it for stopping criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m         \u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_generation_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_assistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massistant_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munused_model_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1244\u001b[0m                 \u001b[0;34mf\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0;34m\" generate arguments will also show up in this list)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['from_pt'] (note: typos in the generate arguments will also show up in this list)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3255.807021,
      "end_time": "2023-02-17T14:27:54.907345",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-17T13:33:39.100324",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bbe0eadca68f40bb991a8aeecf05ebc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4ec896c5f1d40969353fa839c2374e4",
              "IPY_MODEL_b7060a52070144188dd1204f5ba13e03",
              "IPY_MODEL_65628dc09e76437189e48ffecc550dad"
            ],
            "layout": "IPY_MODEL_7f5e1ca5ef89483dab4d152f3b8110ba"
          }
        },
        "b4ec896c5f1d40969353fa839c2374e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ed75de80f0410a8b76ed81b33d3795",
            "placeholder": "",
            "style": "IPY_MODEL_2a2d975c2cf84f02a1b150b0102ee26f",
            "value": "100%"
          }
        },
        "b7060a52070144188dd1204f5ba13e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6debed80914d4724a87f3678b9dfa15a",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e42af46defe463ab32bac514b3f3d94",
            "value": 85
          }
        },
        "65628dc09e76437189e48ffecc550dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9604aae57dbc480688907ed31fa79a20",
            "placeholder": "",
            "style": "IPY_MODEL_dc446c9b5c41477aa57bc9e2cd5719c0",
            "value": "85/85[00:00&lt;00:00,169.61it/s]"
          }
        },
        "7f5e1ca5ef89483dab4d152f3b8110ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ed75de80f0410a8b76ed81b33d3795": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2d975c2cf84f02a1b150b0102ee26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6debed80914d4724a87f3678b9dfa15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e42af46defe463ab32bac514b3f3d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9604aae57dbc480688907ed31fa79a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc446c9b5c41477aa57bc9e2cd5719c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e820966674a44777893b583d2323fb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad5ec44774914c7ca0479e63dc5c6b04",
              "IPY_MODEL_ae81d2f13d2a413d8f8c4bc3a0f46a95",
              "IPY_MODEL_2a2899fe997b48f8916c93dc94726faa"
            ],
            "layout": "IPY_MODEL_60af207cf60e43efaf16258884c955f7"
          }
        },
        "ad5ec44774914c7ca0479e63dc5c6b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd00de1d250b440d82cb6fc54e2e36e3",
            "placeholder": "",
            "style": "IPY_MODEL_b984e50058a34ccd90a12e55b2493ac9",
            "value": "100%"
          }
        },
        "ae81d2f13d2a413d8f8c4bc3a0f46a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7d857bb3cc470db21e04e6e6772e62",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cd4e629c72b4e4eb44cd2fc7b7e5c29",
            "value": 15
          }
        },
        "2a2899fe997b48f8916c93dc94726faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f12f7bd7b84c248e5bd83f6bb1b90e",
            "placeholder": "",
            "style": "IPY_MODEL_1da25bab470b42ef92b528d987a2b2d5",
            "value": "15/15[00:00&lt;00:00,72.67it/s]"
          }
        },
        "60af207cf60e43efaf16258884c955f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd00de1d250b440d82cb6fc54e2e36e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b984e50058a34ccd90a12e55b2493ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c7d857bb3cc470db21e04e6e6772e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd4e629c72b4e4eb44cd2fc7b7e5c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03f12f7bd7b84c248e5bd83f6bb1b90e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da25bab470b42ef92b528d987a2b2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
